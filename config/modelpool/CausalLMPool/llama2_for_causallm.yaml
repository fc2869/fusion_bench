_target_: fusion_bench.modelpool.CausalLMPool
_recursive_: false
# each model should have a name and a path, and the model is loaded from the path
# this is equivalent to `AutoModelForCausalLM.from_pretrained(path)`
models:
  _pretrained_:
    _target_: transformers.LlamaForCausalLM.from_pretrained
    # pretrained_model_name_or_path: /data2/shared_resources/models/llama2/hf/llama-2-7b
    pretrained_model_name_or_path: meta-llama/Llama-2-7b-hf
  expert_1:
    _target_: transformers.LlamaForCausalLM.from_pretrained
    # pretrained_model_name_or_path: /data2/users/fcyin/composition_large_model_ckpts/finetuned/pen_pe
    # pretrained_model_name_or_path: /scratch/10269/fcyin/finetuned_ckpts/full_ckpts/pen_copynext
    # pretrained_model_name_or_path: /scratch/10269/fcyin/finetuned_ckpts/full_ckpts/perm_pem
    pretrained_model_name_or_path: /scratch/10269/fcyin/finetuned_ckpts/full_ckpts/skillmix_atomic_literary
  expert_2:
    _target_: transformers.LlamaForCausalLM.from_pretrained
    # pretrained_model_name_or_path: /data2/users/fcyin/composition_large_model_ckpts/finetuned/pen_copynext
    # pretrained_model_name_or_path: /scratch/10269/fcyin/finetuned_ckpts/full_ckpts/pen_pe
    pretrained_model_name_or_path: /scratch/10269/fcyin/finetuned_ckpts/full_ckpts/skillmix_atomic_rhetorical

model_kwargs:
  torch_dtype: bfloat16
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: meta-llama/Llama-2-7b-hf

# _dataset_loader: fusion_bench.tasks.PEN.pen_load_dataset.load_pen_dataset
# train_datasets:
#   pen_pen:
#     _target_: ${..._dataset_loader}
#     name: pen
#     tokenizer: ${...tokenizer}
#     split: train
# _dataset_loader: fusion_bench.tasks.PERM.perm_load_dataset.load_perm_dataset
# train_datasets:
#   perm_perm:
#     _target_: ${..._dataset_loader}
#     name: perm
#     tokenizer: ${...tokenizer}
#     split: train
_dataset_loader: fusion_bench.tasks.skillmix.skillmix_load_dataset.load_skillmix_dataset
train_datasets:
  skillmix_literary_rhetorical:
    _target_: ${..._dataset_loader}
    name: literary_rhetorical
    tokenizer: ${...tokenizer}
    split: train